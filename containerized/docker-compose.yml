services:
  expert-review-app:
    build:
      context: ..
      dockerfile: containerized/Dockerfile
    container_name: expert-review-system
    ports:
      - "${PYTHON_PORT:-5000}:5000"
      - "${FRONTEND_PORT:-8000}:8000"
    environment:
      # ML Model Configuration
      - MODEL_NAME=${MODEL_NAME:-nlptown/bert-base-multilingual-uncased-sentiment}
      - MODEL_CACHE_DIR=/app/models

      # Recommendation Thresholds
      - HIGHLY_LIKELY_THRESHOLD=${HIGHLY_LIKELY_THRESHOLD:-0.8}
      - WORTH_TRYING_THRESHOLD=${WORTH_TRYING_THRESHOLD:-0.6}
      - PROCEED_CAUTION_THRESHOLD=${PROCEED_CAUTION_THRESHOLD:-0.4}

      # External APIs (Optional - leave empty if not needed)
      - IMDB_API_KEY=${IMDB_API_KEY:-}
      - STEAM_API_KEY=${STEAM_API_KEY:-}
      - METACRITIC_API_KEY=${METACRITIC_API_KEY:-}

      # Rate Limiting
      - RATE_LIMIT_MAX=${RATE_LIMIT_MAX:-100}
      - RATE_LIMIT_WINDOW=${RATE_LIMIT_WINDOW:-60}

      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FORMAT=${LOG_FORMAT:-json}

      # Application
      - ENABLE_MOCK_DATA=${ENABLE_MOCK_DATA:-true}

      # Optional: Hybrid AI via Ollama (LLMs)
      - OLLAMA_ENABLED=${OLLAMA_ENABLED:-false}
      - OLLAMA_SMALL_URL=${OLLAMA_SMALL_URL:-http://ai-ollama-small:11435}
      - OLLAMA_LARGE_URL=${OLLAMA_LARGE_URL:-http://ai-ollama-large:11436}
      - OLLAMA_TIMEOUT=${OLLAMA_TIMEOUT:-60}

    volumes:
      # Persistent data storage for user preferences
      - ./data:/app/data
      # Note: Model cache is baked into the Docker image (669MB BERT model pre-downloaded)
      # Removed volume mount to use pre-downloaded model from image

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

    # Resource limits (adjust based on your needs)
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G
        reservations:
          cpus: "1"
          memory: 1G

  # Optional: Local LLM runtimes via Ollama for hybrid AI
  ollama-small:
    image: ollama/ollama:latest
    container_name: ai-ollama-small
    expose:
      - "11435"
    entrypoint:
      - /bin/sh
      - -c
      - ollama serve & sleep 5 && ollama run gemma3:1b 'Say hello, I am ready.' --format json || true && wait
    volumes:
      # Host path for Ollama models (ensure this drive has ample space)
      - G:\\Projects\\AI-Models:/root/.ollama/models
    gpus: all
    restart: unless-stopped
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=1h
      - OLLAMA_BATCH_SIZE=4
      - OLLAMA_CONTEXT_LENGTH=4096
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11435/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  ollama-large:
    image: ollama/ollama:latest
    container_name: ai-ollama-large
    expose:
      - "11436"
    entrypoint:
      - /bin/sh
      - -c
      - ollama serve & sleep 5 && ollama run qwen2.5:32b 'Say hello, I am ready.' --format json || true && wait
    volumes:
      - G:\\Projects\\AI-Models:/root/.ollama/models
    gpus: all
    restart: unless-stopped
    environment:
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_KEEP_ALIVE=1h
      - OLLAMA_BATCH_SIZE=2
      - OLLAMA_CONTEXT_LENGTH=4096
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11436/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

# Optional: Use networks for better isolation
networks:
  default:
    name: expert-review-network
# Optional: Named volumes for better management
# volumes:
#   data:
#     driver: local
#   models:
#     driver: local
